{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import formulaEvo\n",
    "\n",
    "def getCSVData(csvfile):\n",
    "    print(f\"Processing file: {csvfile}\")\n",
    "    fileData = pd.read_csv(csvfile, sep=';', decimal=',')\n",
    "    fileData = fileData.iloc[2:].reset_index(drop=True)  # Remove first 2 lines\n",
    "\n",
    "    stationData = {}\n",
    "\n",
    "    stationData['time'] = pd.to_datetime(fileData['TIMESTAMP'], format='%d.%m.%Y %H:%M')\n",
    "    stationData['temp'] = fileData['AirTC_Avg'].astype(str).str.replace(',', '.').astype(float)\n",
    "    stationData['humi'] = fileData['RH_Avg'].astype(str).str.replace(',', '.').astype(float)\n",
    "    stationData['light'] = (fileData['SlrW_Avg'].astype(str).str.replace(',', '.').astype(float)*10*60)/1000000  # von W/m² zu Ws/m² (10min Mittelwert), dann zu MJ/m²\n",
    "\n",
    "    #NAN werte ersetzten und finden\n",
    "    for key in stationData: \n",
    "        # Finde NaN-Positionen vor dem Füllen\n",
    "        nan_positions = stationData[key].isnull()\n",
    "        if nan_positions.any():\n",
    "            nan_indices = nan_positions[nan_positions].index.tolist()\n",
    "            print(f\"NaN values found in {key} at positions: {nan_indices}\")\n",
    "            print(f\"Total NaN values in {key}: {nan_positions.sum()}\")\n",
    "        \n",
    "        stationData[key] = stationData[key].ffill(limit=1)\n",
    "        \n",
    "        # Prüfe nach dem Füllen auf verbleibende NaN-Werte\n",
    "        remaining_nan = stationData[key].isnull()\n",
    "        if remaining_nan.any():\n",
    "            remaining_indices = remaining_nan[remaining_nan].index.tolist()\n",
    "            print(f\"Remaining NaN values in {key} after forward fill at positions: {remaining_indices}\")\n",
    "            raise ValueError(f\"More than one NaN value in a row in {key} detected at positions: {remaining_indices}\")\n",
    "\n",
    "    # Trenne data in einzelne Tage auf\n",
    "    stationDays = []\n",
    "    n = 0\n",
    "    for day, group in pd.DataFrame(stationData).groupby(pd.to_datetime(stationData['time']).dt.date):\n",
    "        #lasse nur daten von Kalenderwoche 18 bis 39 zu\n",
    "        if day.strftime('%W') >= '18' and day.strftime('%W') <= '39':\n",
    "            #rohdaten der groupe überprüfen auf lücken\n",
    "            stationDays.append({\n",
    "                'date': day,\n",
    "                'temp_diff_day': max(group['temp'].to_numpy()) - min(group['temp'].to_numpy()),\n",
    "                'time': group['time'],\n",
    "                'temp': group['temp'].to_numpy(),\n",
    "                'light': group['light'].to_numpy(),\n",
    "                'humi': group['humi'].to_numpy()\n",
    "            })\n",
    "\n",
    "    #Erechnen der täglichen Temperartur Differenz\n",
    "    temp_avg_year = np.mean([day['temp_diff_day'] for day in stationDays])\n",
    "    print(f\"Average daily temperature difference from Tepfenhardt data: {temp_avg_year:.2f} °C\")\n",
    "\n",
    "    #Analysiere die Evaporationen\n",
    "    for day in stationDays:\n",
    "        evaporation = formulaEvo.calculate_evaporation_solar(\n",
    "            day['date'],\n",
    "            day['temp_diff_day'],\n",
    "            formulaEvo.krKorean(temp_avg_year),\n",
    "            day['light']\n",
    "        )\n",
    "        day['evo'] = evaporation\n",
    "    return stationDays\n",
    "\n",
    "\n",
    "def seconds_since_midnight(timestamps):\n",
    "    \"\"\"Gibt die Tageszeit in Sekunden (0-86400) für ein datetime-Objekt zurück.\"\"\"\n",
    "    midnight = timestamps.dt.normalize()\n",
    "    return (timestamps - midnight).dt.total_seconds()\n",
    "\n",
    "def make_drain_curve(total_drain, samples):\n",
    "    # nach 10 min 70%\n",
    "    # nach 20 min 90%\n",
    "    # nach 30 min 100%\n",
    "    # erstellt eine Drainkurve in der Länge der Episoden samples\n",
    "    values = [0, 0.7*total_drain, 0.9*total_drain]\n",
    "    # Schneide oder fülle auf die gewünschte Länge\n",
    "    if samples <= len(values):\n",
    "        return values[:samples]\n",
    "    else:\n",
    "        values += [total_drain] * (samples - len(values))\n",
    "        return values\n",
    "\n",
    "def split_day_by_evo(day, threshold=2.0):\n",
    "    #episode auschneiden länge = 5l(ideal)+ Überschuss Drain in l([-0,75,+2]l)\n",
    "    #dann wert von Überschusss (+-2l) durch ausgleichskurve in Drainkurve umwandeln\n",
    "    # nach 10 min 70%\n",
    "    # nach 20 min 90%\n",
    "    # nach 30 min 100%\n",
    "    \"\"\"\n",
    "    Teilt jeden Tag in tepfi_days in Episoden, sodass jede Episode ca. threshold mm (z.B. 2mm) kumulierte Verdunstung enthält.\n",
    "    Jede Episode enthält die kompletten Wertearrays (temp, humi, light, etc.) für den jeweiligen Abschnitt.\n",
    "    Rückgabe: Liste von Episoden, jede Episode ist ein Dict mit den Feldern wie ein Tag, aber nur für den Episodenbereich.\n",
    "    \"\"\"\n",
    "    episodes = []\n",
    "    evo = np.asarray(day['evo'])\n",
    "    cumsum = np.cumsum(evo)\n",
    "    start_idx = 0\n",
    "    last_cum = cumsum[0] if len(cumsum) > 0 else 0\n",
    "    drain = np.random.uniform(-0.75, 2.0)\n",
    "    threshold_total = threshold + drain  # füge zufälligen Überschuss hinzu\n",
    "    if 'date' in day:\n",
    "        del day['date']\n",
    "    if 'temp_diff_day' in day:\n",
    "        del day['temp_diff_day']\n",
    "    for i in range(1, len(evo)):\n",
    "        if cumsum[i] - last_cum >= threshold_total:\n",
    "            epi = {}\n",
    "            for key in day:\n",
    "                epi[key] = day[key][start_idx:i]\n",
    "            epi['drain'] = make_drain_curve(threshold_total, i - start_idx)\n",
    "            episodes.append(epi)\n",
    "            start_idx = i\n",
    "            last_cum = cumsum[i]\n",
    "            new_drain = np.random.uniform(-0.75, 2.0)\n",
    "            threshold_total = threshold + new_drain  # neuer zufälliger Überschuss\n",
    "    # letzte Episode bis zum Ende\n",
    "    if start_idx < len(evo):\n",
    "        epi = {}\n",
    "        for key in day:\n",
    "            epi[key] = day[key][start_idx:]\n",
    "        episodes.append(epi)\n",
    "    return episodes\n",
    "\n",
    "#save Episodes from data\n",
    "def save_Episode(baseFolder, month, day, station, episodes):\n",
    "    if not os.path.exists(baseFolder):\n",
    "        os.makedirs(baseFolder)\n",
    "\n",
    "    for i, epi in enumerate(episodes):\n",
    "        fileName = os.path.join(baseFolder, f\"{station}-{month}-{day}-Epi{i}.csv\")\n",
    "        with open(fileName, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            for j in range(len(epi['time'])):\n",
    "                observation = [\n",
    "                    epi['doy'],\n",
    "                    epi['abstime'][j],\n",
    "                    epi['time'][j],\n",
    "                    epi['temp'][j],\n",
    "                    epi['humi'][j],\n",
    "                    epi['light'][j],\n",
    "                    epi['drain'][j],\n",
    "                    epi['timeToIrri'][j]\n",
    "                ]\n",
    "                writer.writerow(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc39390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../CSV AgrarMeteo\\Bavendorf.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NilsWindows\\AppData\\Local\\Temp\\ipykernel_11008\\3015966653.py:9: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,13,14,15,16,17,18,19,20,21,22,23,24,25,30,31,32,36,37,38,39,40,50,51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  fileData = pd.read_csv(csvfile, sep=';', decimal=',')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "More than one NaN value in a row in temp detected.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m csvfile\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m     csvfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(csvfolder, csvfile)\n\u001b[1;32m----> 8\u001b[0m     stationDays \u001b[38;5;241m=\u001b[39m \u001b[43mgetCSVData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsvfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m day \u001b[38;5;129;01min\u001b[39;00m stationDays:\n\u001b[0;32m     10\u001b[0m         epiDay \u001b[38;5;241m=\u001b[39m split_day_by_evo(day, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m, in \u001b[0;36mgetCSVData\u001b[1;34m(csvfile)\u001b[0m\n\u001b[0;32m     21\u001b[0m     stationData[key] \u001b[38;5;241m=\u001b[39m stationData[key]\u001b[38;5;241m.\u001b[39mffill(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stationData[key]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore than one NaN value in a row in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Trenne data in einzelne Tage auf\u001b[39;00m\n\u001b[0;32m     26\u001b[0m stationDays \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: More than one NaN value in a row in temp detected."
     ]
    }
   ],
   "source": [
    "csvfolder = \"../CSV AgrarMeteo\"\n",
    "savePath = \"../episFormula/1. Basic\"\n",
    "episodenAll = []\n",
    "\n",
    "for csvfile in os.listdir(csvfolder):\n",
    "    if csvfile.endswith(\".csv\"):\n",
    "        csvfile = os.path.join(csvfolder, csvfile)\n",
    "        stationDays = getCSVData(csvfile)\n",
    "        for day in stationDays:\n",
    "            epiDay = split_day_by_evo(day, threshold=5.0)\n",
    "            episodenRaw = epiDay[1:-1]  # ggf. [1:-1] für Episodenfilter\n",
    "            episoden = []\n",
    "            for epiRaw in episodenRaw:\n",
    "                episode = {}\n",
    "                episode['doy'] = pd.Timestamp(epiRaw['time'].iloc[0]).dayofyear / 280  # max 280 Tage (KW39=273doy)\n",
    "                episode['abstime'] = np.array(seconds_since_midnight(epiRaw['time'])) / (24*3600)\n",
    "                episode['time'] = np.array((epiRaw['time'] - epiRaw['time'].iloc[0]).dt.total_seconds()) / (60 * 300) # in min und max 300 min Episode\n",
    "                episode['temp'] = np.array(epiRaw['temp']) / 25 # 25°C max\n",
    "                episode['humi'] = np.array(epiRaw['humi']) / 100 # 100% max\n",
    "                episode['light'] = epiRaw['light'].cumsum() / 20 # 20 MJ/testfläche max\n",
    "                episode['timeToIrri'] = np.array((epiRaw['time'].iloc[-1] - epiRaw['time']).dt.total_seconds()) / 60 # in min\n",
    "                episode['drain'] = np.array(epiRaw['drain']) / 7 # 7l max\n",
    "                episoden.append(episode)\n",
    "            episodenAll.append(episoden)\n",
    "            month = pd.Timestamp(day['time'].iloc[0]).month\n",
    "            daynum = pd.Timestamp(day['time'].iloc[0]).day\n",
    "            save_Episode(savePath, str(month), str(daynum), episoden)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
